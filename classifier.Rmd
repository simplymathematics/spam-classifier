---
title: "DATA 607 Project 4"
author: simplymathematics
date: "November 2, 2018"
output: html_document
---

```
PROJECT 4: Document Classification
It can be useful to be able to classify new "test" documents using already classified "training" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  

For this project, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder). 
```
# Dependencies
As always, we start with dependencies
```{r}
require(tm)
require(tidyverse)
```

Here, we gather our data
```{r}
file.path <- c("easy_ham/", "spam/", "spam_2/", "spam_3/")
```

#Data Cleaning:

First I read each file into a corpus using the ```tm``` package.
```{r}
spam <- file.path[2] %>% DirSource() %>% VCorpus() 
ham <- file.path[1] %>% DirSource() %>% VCorpus()
test <- file.path[3] %>% DirSource() %>% VCorpus()
spam
ham
test
```

Then, we had to clean each email. So, I made a function that could do it for each type.
```{r}
list <- c("date", "deliveredto", "received", "subject", "localhost", "returnpath")
clean <- function(tmp){
    tmp <- tmp %>% tm_map(content_transformer(PlainTextDocument))
    tmp <- tmp %>% tm_map(content_transformer(function(x) gsub(x, pattern="[^a-zA-Z/d/s:]", replacement=" "))) #deletes all weird symbols
    tmp <- tmp %>% tm_map(content_transformer(function(x) iconv(enc2utf8(x), sub = "byte")))
    tmp <- tmp %>% tm_map(removePunctuation) #remove punctuation
    tmp <- tmp %>% tm_map(removeNumbers)     #remove numbers
    tmp <- tmp %>% tm_map(content_transformer(tolower)) #change to low case
    tmp <- tmp %>% tm_map(stemDocument) #truncates words
    tmp <- tmp %>% tm_map(content_transformer(function(x) removeWords(x, words = list)))
    tmp <- tmp %>% tm_map(removeWords, stopwords('english'))
    return(tmp)
}
```

Then, we make a vector, both, that contains both.
```{r}
clean.ham <- ham %>% clean()
clean.spam <- spam %>% clean()
clean.test <- test %>% clean()
```
Next, I created a document term matrix and removed sparse terms for the whole set, the ham set, and the spam set.
```{r}
sparsity = .93 #picked through testing such that in maximizes the distance between ham and spam
final.ham <- clean.ham %>% DocumentTermMatrix() %>% removeSparseTerms(sparsity)
final.spam <- clean.spam %>% DocumentTermMatrix() %>% removeSparseTerms(sparsity)
final.test <- clean.test %>% DocumentTermMatrix() %>% removeSparseTerms(sparsity)

```
Finally, I made a matrix for each frequency table.
```{r}
# Ham
ham.freq <- final.ham %>% as.matrix %>% colSums() %>% as.data.frame() %>% rownames_to_column("string")
colnames(ham.freq) <- c('string', 'count')

# Spam
spam.freq <- final.spam %>% as.matrix %>% colSums() %>% as.data.frame() %>% rownames_to_column("string")
colnames(spam.freq) <- c('string', 'count')

#Test
test.freq <- final.test %>% as.matrix %>% colSums() %>% as.data.frame() %>% rownames_to_column("string")
colnames(test.freq) <- c('string', 'count')
```
Then, I merged the tables and converted the 'NAs' to 0. 
```{r}
# Ham and Spam 1
table <- merge(x = ham.freq, y = spam.freq, by = 'string', all.x = TRUE)
table <- table %>% mutate_all(funs(replace(., is.na(.), 0))) %>% as.data.frame()
colnames(table) <- c('string', 'ham.count', 'spam1.count')
row.names(table) <- NULL


# Ham, Spam 1, and Spam 2
table2 <- merge(x = table, y = test.freq, by = 'string', all.x = TRUE)
table2 <- table2 %>% mutate_all(funs(replace(., is.na(.), 0))) %>% as.data.frame()
colnames(table2) <- c('string', 'ham.count', 'spam1.count', 'spam2.count')
row.names(table2) <- NULL



```

Then, I did a $\chi^2$-test to see if the there was a significant difference between these two categories.
```{r}
value1 <- chisq.test(table$ham.count, table$spam1.count)
value2 <- chisq.test(table2$ham.count, table2$spam2.count)
value3 <- chisq.test(table2$spam1.count, table2$spam2.count)
value1
value2
value3
```
As we can see from the test, a naive chi-square test of the word frequencies can work as a classifier. 

## An Example Implementation
Below we've implemented an example classifier that tests a file from the spam 2 folder. In this case it is successful, but the correlation values are very close. Therefore, this model may be over-fitting.
```{r}
foo <- file.path[4] %>% DirSource() %>% VCorpus()


foo <- foo %>% clean()
foo <- foo %>% DocumentTermMatrix() 
foo <- foo %>% as.matrix %>% colSums() %>% as.data.frame() %>% rownames_to_column("string")
colnames(foo) <- c('string', 'test.case')
table3 <- merge(x = table, y = foo, by = 'string', all.x=TRUE)
table3 <- table3 %>% mutate_all(funs(replace(., is.na(.), 0))) %>% as.data.frame()
a <- cor(table3$ham.count, table3$test.case)
b <- cor(table3$spam1.count, table3$test.case)
if (a > b){
  print("test case is not spam")
} else if (a == b){
  print("we cannot draw a conclusion")
} else{
  print("test case is spam")
}

```

